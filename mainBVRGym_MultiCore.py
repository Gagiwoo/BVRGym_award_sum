# mainBVRGym_MultiCore.py - ìµœì¢… ìˆ˜ì •ë³¸

import os
import json
import argparse, time
from jsb_gym.environments import evasive, bvrdog
import numpy as np
from enum import Enum
from torch.utils.tensorboard import SummaryWriter
import torch
import multiprocessing
from jsb_gym.RL.ppo import Memory, PPO
from jsb_gym.environments.config import BVRGym_PPO1, BVRGym_PPO2, BVRGym_PPODog
from numpy.random import seed
from jsb_gym.TAU.config import aim_evs_BVRGym, f16_evs_BVRGym, aim_dog_BVRGym, f16_dog_BVRGym

def init_pool():
    seed()

class Maneuvers(Enum):
    Evasive = 0
    Crank = 1

def runPPO(args):
    # --- ì‹œë‚˜ë¦¬ì˜¤ ë° í™˜ê²½ ì„¤ì • ---
    if args['track'] == 'M1':
        from jsb_gym.RL.config.ppo_evs_PPO1 import conf_ppo
        env_config = BVRGym_PPO1
        env = evasive.Evasive(env_config, args, aim_evs_BVRGym, f16_evs_BVRGym)
        torch_save_path = 'jsb_gym/logs/RL/M1.pth'
        state_scale = 1
    elif args['track'] == 'M2':
        from jsb_gym.RL.config.ppo_evs_PPO2 import conf_ppo
        env_config = BVRGym_PPO2
        env = evasive.Evasive(env_config, args, aim_evs_BVRGym, f16_evs_BVRGym)
        torch_save_path = 'jsb_gym/logs/RL/M2.pth'
        state_scale = 2
    elif args['track'] == 'Dog':
        from jsb_gym.RL.config.ppo_evs_PPO_BVRDog import conf_ppo
        env_config = BVRGym_PPODog
        env = bvrdog.BVRDog(env_config, args, aim_dog_BVRGym, f16_dog_BVRGym)
        torch_save_path = 'jsb_gym/logs/RL/Dog.pth'
        state_scale = 1
    # ... ë‹¤ë¥¸ ì‹œë‚˜ë¦¬ì˜¤ ì¶”ê°€ ê°€ëŠ¥

    # --- ìˆ˜ì • 1: ì»¤ë§¨ë“œ ë¼ì¸ ì¸ìë¥¼ conf ê°ì²´ì— ë°˜ì˜ ---
    method = args.get('method', 'baseline')
    conf_ppo.setdefault('general', {})
    conf_ppo['general']['method'] = method
    
    # PPO ì•Œê³ ë¦¬ì¦˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
    conf_ppo['entropy_weight'] = args['entropy_w']
    conf_ppo['shaping_w'] = args['shaping_w'] # shaping ê°€ì¤‘ì¹˜ë„ confë¥¼ í†µí•´ ì „ë‹¬

    # í™˜ê²½ ë³´ìƒ ê°€ì¤‘ì¹˜ ì„¤ì • (evasive.pyì—ì„œ ì‚¬ìš©)
    if hasattr(env_config, 'reward_weights'):
        env_config.reward_weights['tactical'] = args['tactical_w']
        env_config.reward_weights['shaping'] = args['shaping_w']
        env_config.reward_weights['entropy'] = args['entropy_w']

    # --- ìˆ˜ì • 2: í…ì„œë³´ë“œ Writer ë™ì  ìƒì„± ---
    run_name = args.get('run_name')
    if not run_name:
        run_name = f"{args['track']}_{method}_tw{args['tactical_w']}_sw{args['shaping_w']}_ew{args['entropy_w']}_{time.strftime('%Y%m%d-%H%M%S')}"
    writer = SummaryWriter(f"runs/{run_name}")

    # --- PPO ë° ë©€í‹°í”„ë¡œì„¸ì‹± í’€ ì´ˆê¸°í™” ---
    state_dim = env.observation_space
    action_dim = env.action_space.shape[1]
    memory = Memory()
    ppo = PPO(state_dim * state_scale, action_dim, conf_ppo, use_gpu=args.get('gpu', False))
    pool = multiprocessing.Pool(processes=int(args['cpu_cores']), initializer=init_pool)

    print(f"--- í•™ìŠµ ì‹œì‘: {run_name} ---")

    for i_episode in range(1, args['Eps'] + 1):
        ppo_policy = ppo.policy.state_dict()
        ppo_policy_old = ppo.policy_old.state_dict()
        input_data = [(args, ppo_policy, ppo_policy_old, conf_ppo, state_scale) for _ in range(args['cpu_cores'])]
        
        running_rewards = []
        all_infos = []

        # ë³‘ë ¬ í•™ìŠµ ì‹¤í–‰
        results = pool.map(train, input_data)
        
        # ê²°ê³¼ ì§‘ê³„
        for tmp in results:
            memory.actions.extend(tmp[0])
            memory.states.extend(tmp[1])
            memory.logprobs.extend(tmp[2])
            memory.rewards.extend(tmp[3])
            memory.is_terminals.extend(tmp[4])
            running_rewards.append(tmp[5])
            all_infos.append(tmp[6]) # ğŸ‘ˆ ìˆ˜ì • 3: info ë”•ì…”ë„ˆë¦¬ ìˆ˜ì§‘

        # ì •ì±… ì—…ë°ì´íŠ¸
        ppo.update(memory, to_tensor=True) 
        memory.clear_memory()
        torch.cuda.empty_cache()

        # --- ìˆ˜ì • 4: ì „ì²´ ë¡œê¹… ë¡œì§ ë³€ê²½ ---
        # 1. ê¸°ë³¸ í•™ìŠµ ì§€í‘œ ê·¸ë£¹
        writer.add_scalar("Metrics/Running_Reward", np.mean(running_rewards), i_episode)
        writer.add_scalar("Agent/Policy_Entropy", ppo.get_entropy(), i_episode)

        # 2. ìƒì„¸ í‰ê°€ì§€í‘œ ê·¸ë£¹ (ìƒˆë¡œìš´ ì§€í‘œë“¤)
        valid_infos = [info for info in all_infos if 'survival' in info]
        if valid_infos:
            avg_survival = np.mean([info['survival'] for info in valid_infos])
            
            # íšŒí”¼ ì„±ê³µí•œ ê²½ìš°ë§Œ í•„í„°ë§
            successful_evasions = [info for info in valid_infos if info['survival'] > 0 and not np.isnan(info['evasion_time'])]
            
            avg_min_cpa = np.mean([info['min_cpa'] for info in successful_evasions]) if successful_evasions else 0
            avg_evasion_time = np.mean([info['evasion_time'] for info in successful_evasions]) if successful_evasions else 0
            
            writer.add_scalar("Performance/Survival_Rate", avg_survival, i_episode)
            writer.add_scalar("Performance/Min_CPA", avg_min_cpa, i_episode)
            writer.add_scalar("Performance/Evasion_Time", avg_evasion_time, i_episode)

        # 3. ë³´ìƒ êµ¬ì„±ìš”ì†Œ ê·¸ë£¹ (í™˜ê²½ì— last_reward_componentsê°€ ìˆëŠ” ê²½ìš°)
        if hasattr(env, 'last_reward_components'):
            for k, v in env.last_reward_components.items():
                clean_key = k.replace('reward_', '').capitalize()
                writer.add_scalar(f"Rewards/{clean_key}", v, i_episode)
        
            if i_episode % 100 == 0:
                # 'results'ì—ì„œ ê¶¤ì  ë°ì´í„° ìˆ˜ì§‘
                trajectories = [tmp[7] for tmp in results]

                # ì²« ë²ˆì§¸ ì›Œì»¤ì˜ ê¶¤ì ë§Œ ì €ì¥
                if trajectories:
                    trajectory_to_save = trajectories[0] 
                    save_filename = f"trajectories/{run_name}_ep{i_episode}.json"

                    # trajectories í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
                    os.makedirs("trajectories", exist_ok=True)

                    with open(save_filename, 'w') as f:
                        json.dump(trajectory_to_save, f, indent=4)
                    print(f"Episode {i_episode}: Trajectory saved to {save_filename}")


        # ëª¨ë¸ ì €ì¥
        if i_episode % 500 == 0:
            torch.save(ppo.policy.state_dict(), f"{torch_save_path}_ep{i_episode}.pth")
            print(f"Episode {i_episode}: Model saved.")

    pool.close()
    pool.join()
    writer.close()
    print(f"--- í•™ìŠµ ì¢…ë£Œ: {run_name} ---")


def train(packed_args):
    # ì¸ì ì–¸íŒ¨í‚¹
    args, ppo_policy, ppo_policy_old, conf_ppo, state_scale, = packed_args

    # ê° í”„ë¡œì„¸ìŠ¤ì—ì„œ í™˜ê²½ ë‹¤ì‹œ ìƒì„±
    if args['track'] == 'M1':
        from jsb_gym.environments.config import BVRGym_PPO1
        env = evasive.Evasive(BVRGym_PPO1, args, aim_evs_BVRGym, f16_evs_BVRGym)
    elif args['track'] == 'M2':
        from jsb_gym.environments.config import BVRGym_PPO2
        env = evasive.Evasive(BVRGym_PPO2, args, aim_evs_BVRGym, f16_evs_BVRGym)
    elif args['track'] == 'Dog':
        from jsb_gym.environments.config import BVRGym_PPODog
        env = bvrdog.BVRDog(BVRGym_PPODog, args, aim_dog_BVRGym, f16_dog_BVRGym)
    elif args['track'] == 'DogR':
        from jsb_gym.environments.config import BVRGym_PPODog
        env = bvrdog.BVRDog(BVRGym_PPODog, args, aim_dog_BVRGym, f16_dog_BVRGym)

    maneuver = Maneuvers.Evasive
    memory = Memory()
    state_dim = env.observation_space
    action_dim = env.action_space.shape[1]
    ppo = PPO(state_dim * state_scale, action_dim, conf_ppo, use_gpu=False)

    ppo.policy.load_state_dict(ppo_policy)
    ppo.policy_old.load_state_dict(ppo_policy_old)
    ppo.policy.eval()
    ppo.policy_old.eval()

    running_reward = 0.0
    final_info = {}

    for _ in range(args['eps']):
        # --- ìƒíƒœ ì´ˆê¸°í™” ---
        if args['track'] in ['M1', 'M2']:
            state_block = env.reset(True, True)
            state = state_block['aim1'] if args['track'] == 'M1' else np.concatenate((state_block['aim1'][0], state_block['aim2'][0]))
        else:
            state = env.reset()

        # --- ì—í”¼ì†Œë“œ ì‹¤í–‰ ---
        done = False
        while not done:
            act = ppo.select_action(state, memory)
            action = np.zeros(3)
            action[0], action[1] = act[0], act[1]
            if args['track'] in ['M1', 'M2']:
                action[2] = 1 # Evasive ì‹œë‚˜ë¦¬ì˜¤ì—ì„œëŠ” ìŠ¤ë¡œí‹€ ê³ ì • ë“±

            # env.step() í˜¸ì¶œ
            next_state_block, reward, done, info = env.step(action, action_type=maneuver.value)
            
            # ë‹¤ìŒ ìƒíƒœ ê²°ì •
            if args['track'] == 'M1':
                state = next_state_block['aim1']
            elif args['track'] == 'M2':
                state = np.concatenate((next_state_block['aim1'], next_state_block['aim2']))
            else:
                state = next_state_block

            memory.rewards.append(reward)
            memory.is_terminals.append(done)

        running_reward += reward
        final_info = info # ë§ˆì§€ë§‰ ìŠ¤í…ì˜ info ì €ì¥

    running_reward /= args['eps']
    
    # ë””ë°”ì´ìŠ¤ í˜¸í™˜ì„±ì„ ìœ„í•´ numpyë¡œ ë³€í™˜
    actions = [i.cpu().detach().numpy() for i in memory.actions]
    states = [i.cpu().detach().numpy() for i in memory.states]
    logprobs = [i.cpu().detach().numpy() for i in memory.logprobs]

    return [actions, states, logprobs, memory.rewards, memory.is_terminals, running_reward, final_info, env.trajectory_log]

def interactive_menu():
    """ì‚¬ìš©ìë¡œë¶€í„° ì¸í„°ë™í‹°ë¸Œí•˜ê²Œ ì‹¤í—˜ ì„¤ì •ì„ ì…ë ¥ë°›ëŠ” í•¨ìˆ˜"""
    args = {}
    print("==============================================")
    print(" BVRGym ê°•í™”í•™ìŠµ ì‹œë®¬ë ˆì´í„°")
    print("==============================================")

    # --- 1. íŠ¸ë™(ì‹œë‚˜ë¦¬ì˜¤) ì„ íƒ ---
    while True:
        print("\n[1] í•™ìŠµí•  ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
        print("  1: M1 (ë‹¨ì¼ ë¯¸ì‚¬ì¼ íšŒí”¼)")
        print("  2: M2 (ë‹¤ì¤‘ ë¯¸ì‚¬ì¼ íšŒí”¼)")
        print("  3: Dog (BVR êµì „)")
        track_choice = input(">> ì„ íƒ (ë²ˆí˜¸ ì…ë ¥): ")
        if track_choice == '1':
            args['track'] = 'M1'
            break
        elif track_choice == '2':
            args['track'] = 'M2'
            break
        elif track_choice == '3':
            args['track'] = 'Dog'
            break
        else:
            print("ğŸš¨ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. 1, 2, 3 ì¤‘ í•˜ë‚˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

    # --- 2. ê¸°ë²•(ë©”ì†Œë“œ) ì„ íƒ ---
    while True:
        print("\n[2] ì ìš©í•  ê¸°ë²•ì„ ì„ íƒí•˜ì„¸ìš”:")
        print("  1: baseline (ìƒì¡´ ë³´ìƒë§Œ ì‚¬ìš©)")
        print("  2: shaping (Shaping ë³´ìƒ ì¶”ê°€)")
        print("  3: entropy (Entropy ë³´ë„ˆìŠ¤ ê°•í™”)")
        print("  4: proposed (ì œì•ˆí•˜ëŠ” í†µí•© ë³´ìƒ í”„ë ˆì„ì›Œí¬)")
        method_choice = input(">> ì„ íƒ (ë²ˆí˜¸ ì…ë ¥): ")
        if method_choice == '1':
            args['method'] = 'baseline'
            break
        elif method_choice == '2':
            args['method'] = 'shaping'
            break
        elif method_choice == '3':
            args['method'] = 'entropy'
            break
        elif method_choice == '4':
            args['method'] = 'proposed'
            break
        else:
            print("ğŸš¨ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. 1, 2, 3, 4 ì¤‘ í•˜ë‚˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")

    # --- 3. ì„¸ë¶€ ê°€ì¤‘ì¹˜ ì„¤ì • ---
    print("\n[3] ì„¸ë¶€ ê°€ì¤‘ì¹˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. (ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•˜ë ¤ë©´ Enter)")
    
    # ê¸°ë³¸ê°’ ì„¤ì •
    args['shaping_w'] = 0.05
    args['entropy_w'] = 0.01
    args['tactical_w'] = 0.3

    if args['method'] in ['shaping', 'proposed']:
        sw_input = input(f">> Shaping ê°€ì¤‘ì¹˜ ì…ë ¥ (ê¸°ë³¸ê°’: {args['shaping_w']}): ")
        if sw_input: args['shaping_w'] = float(sw_input)
    
    if args['method'] in ['entropy', 'proposed']:
        ew_input = input(f">> Entropy ê°€ì¤‘ì¹˜ ì…ë ¥ (ê¸°ë³¸ê°’: {args['entropy_w']}): ")
        if ew_input: args['entropy_w'] = float(ew_input)

    if args['method'] == 'proposed':
        tw_input = input(f">> Tactical ê°€ì¤‘ì¹˜ ì…ë ¥ (ê¸°ë³¸ê°’: {args['tactical_w']}): ")
        if tw_input: args['tactical_w'] = float(tw_input)

    # --- 4. ê¸°íƒ€ ì„¤ì • ---
    print("\n[4] ê¸°íƒ€ í•™ìŠµ ì„¤ì •ì„ ì…ë ¥í•©ë‹ˆë‹¤. (ê¸°ë³¸ê°’ì„ ì‚¬ìš©í•˜ë ¤ë©´ Enter)")
    
    cpu_cores_input = input(">> ì‚¬ìš©í•  CPU ì½”ì–´ ìˆ˜ (ê¸°ë³¸ê°’: 12): ")
    args['cpu_cores'] = int(cpu_cores_input) if cpu_cores_input else 12
    
    eps_input = input(">> ë³‘ë ¬ ì—í”¼ì†Œë“œ ìˆ˜ (ê¸°ë³¸ê°’: 3): ")
    args['eps'] = int(eps_input) if eps_input else 3
    
    Eps_input = input(">> ì´ í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜ (ê¸°ë³¸ê°’: 10000): ")
    args['Eps'] = int(Eps_input) if Eps_input else 10000

        # --- 5. GPU ì‚¬ìš© ì—¬ë¶€ ì„ íƒ ---
    while True:
        print("\n[5] GPU(CUDA)ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ê°€ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ?")
        print("   (NVIDIA GPU ë° CUDA í™˜ê²½ ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤)")
        gpu_choice = input(">> ì„ íƒ (y/n): ").lower()
        if gpu_choice == 'y':
            args['gpu'] = True
            break
        elif gpu_choice == 'n':
            args['gpu'] = False
            break
        else:
            print("ğŸš¨ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. 'y' ë˜ëŠ” 'n'ì„ ì…ë ¥í•˜ì„¸ìš”.")

    # argparseì™€ í˜•ì‹ì„ ë§ì¶”ê¸° ìœ„í•œ ë‚˜ë¨¸ì§€ ê¸°ë³¸ê°’ë“¤
    args['vizualize'] = False
    args['run_name'] = ""

    print("\n--- ì„¤ì • ì™„ë£Œ ---")
    for key, value in args.items():
        print(f"  {key}: {value}")
    print("--------------------")
    
    return args


if __name__ == '__main__':
    # ì¸í„°ë™í‹°ë¸Œ ë©”ë‰´ë¥¼ í†µí•´ ì‹¤í—˜ ì„¤ì •ê°’ì„ ë°›ì•„ì˜´
    args = interactive_menu()
    
    # ì„¤ì •ëœ ê°’ìœ¼ë¡œ PPO í•™ìŠµ ì‹¤í–‰
    runPPO(args)